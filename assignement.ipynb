{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Amharic N-gram Language Model Implementation\n",
    "This notebook implements an n-gram language model for Amharic text processing.\n",
    "\n",
    "1. Setup and Imports\n",
    "Import required libraries and set up initial configurations\n",
    "\n",
    "Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Amharic N-gram Language Model Implementation\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "\n",
    "# Set font family to handle Amharic text properly\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount Google Drive and read corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "SAMPLE_DATA_PATH = \"/content/drive/MyDrive/aait-nlp/GPAC_sample.txt\"\n",
    "FULL_DATA_PATH = \"/content/drive/MyDrive/aait-nlp/GPAC.txt\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "STOPWORDS_PATH = \"stopwords.txt\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Core Functions\n",
    "Basic functions for reading and processing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus reading function\n",
    "def read_corpus(file_path, max_lines=None):\n",
    "    \"\"\"Read the corpus file and return a list of cleaned sentences\"\"\"\n",
    "    print(f\"Reading corpus from {file_path}...\")\n",
    "    \n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                sentences.append(line)\n",
    "                line_count += 1\n",
    "                \n",
    "                if max_lines and line_count >= max_lines:\n",
    "                    break\n",
    "                \n",
    "                if line_count % 10000 == 0:\n",
    "                    print(f\"Read {line_count} lines...\")\n",
    "    \n",
    "    print(f\"Loaded {len(sentences)} sentences.\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_stopwords(file_path):\n",
    "    \"\"\"Read Amharic stopwords from file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return set(line.strip() for line in f if line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Stopwords file {file_path} not found. Using empty stopwords list.\")\n",
    "        return set()\n",
    "\n",
    "# Define tokenization functions\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"Tokenize a sentence into words\"\"\"\n",
    "    # Simple tokenization by whitespace for Amharic\n",
    "    return sentence.split()\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"Generate n-grams from a list of tokens\"\"\"\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-gram Analysis Functions\n",
    "Functions for analyzing and logging n-gram patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define n-gram logging function\n",
    "def log_ngram_examples(ngram_counter, n, num_examples=3):\n",
    "    \"\"\"Print example ngrams and their contexts to understand structure\"\"\"\n",
    "    print(f\"\\n=== N-gram Structure Examples (n={n}) ===\")\n",
    "    \n",
    "    # Show most common n-grams\n",
    "    print(f\"Top {num_examples} most common {n}-grams:\")\n",
    "    for ngram, count in ngram_counter.most_common(num_examples):\n",
    "        print(f\"  N-gram: '{' '.join(ngram)}' (Count: {count})\")\n",
    "        \n",
    "        # For n>1, explain conditional probability concept\n",
    "        if n > 1:\n",
    "            context = ' '.join(ngram[:-1])\n",
    "            next_word = ngram[-1]\n",
    "            print(f\"    Context: '{context}' → Next word: '{next_word}'\")\n",
    "            print(f\"    This represents P('{next_word}' | '{context}')\")\n",
    "    \n",
    "    # Show random examples for variety\n",
    "    print(f\"\\nRandom {n}-gram examples:\")\n",
    "    random_samples = random.sample(list(ngram_counter.items()), min(num_examples, len(ngram_counter)))\n",
    "    for ngram, count in random_samples:\n",
    "        print(f\"  N-gram: '{' '.join(ngram)}' (Count: {count})\")\n",
    "        if n > 1:\n",
    "            context = ' '.join(ngram[:-1])\n",
    "            next_word = ngram[-1]\n",
    "            print(f\"    This means after seeing '{context}', '{next_word}' appeared {count} times\")\n",
    "\n",
    "# Define frequency computation function\n",
    "def compute_ngram_frequencies(sentences, n, remove_stopwords=False, stopwords=None):\n",
    "    \"\"\"Compute frequencies of n-grams from a list of sentences\"\"\"\n",
    "    ngram_counter = Counter()\n",
    "    token_count = 0\n",
    "    sentence_with_ngrams = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        token_count += len(tokens)\n",
    "        \n",
    "        if remove_stopwords and stopwords:\n",
    "            original_len = len(tokens)\n",
    "            tokens = [token for token in tokens if token not in stopwords]\n",
    "            removed = original_len - len(tokens)\n",
    "            if removed > 0 and len(tokens) > 0:\n",
    "                print(f\"  Example: Removed {removed} stopwords from: '{' '.join(tokens)}'\") if sentence_with_ngrams < 2 else None\n",
    "        \n",
    "        if len(tokens) >= n:\n",
    "            sentence_with_ngrams += 1\n",
    "            ngrams = generate_ngrams(tokens, n)\n",
    "            ngram_counter.update(ngrams)\n",
    "            \n",
    "            if sentence_with_ngrams <= 2 and not remove_stopwords:\n",
    "                print(f\"\\nExample of tokenizing sentence into {n}-grams:\")\n",
    "                print(f\"  Sentence: '{sentence}'\")\n",
    "                print(f\"  Tokens: {tokens}\")\n",
    "                print(f\"  {n}-grams generated:\")\n",
    "                for i, ngram in enumerate(ngrams[:5]):\n",
    "                    print(f\"    {i+1}. {ngram}\")\n",
    "                if len(ngrams) > 5:\n",
    "                    print(f\"    ... and {len(ngrams)-5} more {n}-grams\")\n",
    "    \n",
    "    print(f\"\\nProcessed {token_count} total tokens across {sentence_with_ngrams} sentences\")\n",
    "    print(f\"Found {len(ngram_counter)} unique {n}-grams out of {sum(ngram_counter.values())} total {n}-grams\")\n",
    "    \n",
    "    return ngram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Probability Functions\n",
    "Functions for computing various probability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability computation functions\n",
    "def compute_ngram_probabilities(ngram_counter, total_ngrams):\n",
    "    \"\"\"Compute probabilities of n-grams\"\"\"\n",
    "    ngram_probs = {ngram: count / total_ngrams for ngram, count in ngram_counter.items()}\n",
    "    \n",
    "    prob_sum = sum(ngram_probs.values())\n",
    "    max_prob = max(ngram_probs.values()) if ngram_probs else 0\n",
    "    min_prob = min(ngram_probs.values()) if ngram_probs else 0\n",
    "    \n",
    "    print(f\"\\nProbability statistics:\")\n",
    "    print(f\"  Sum of all probabilities: {prob_sum} (should be close to 1.0)\")\n",
    "    print(f\"  Maximum probability: {max_prob:.6f}\")\n",
    "    print(f\"  Minimum probability: {min_prob:.6f}\")\n",
    "    print(f\"  Probability range: {max_prob - min_prob:.6f}\")\n",
    "    \n",
    "    return ngram_probs\n",
    "\n",
    "def compute_conditional_probabilities(bigram_counter, unigram_counter):\n",
    "    \"\"\"Compute conditional probabilities P(word2|word1) using bigrams\"\"\"\n",
    "    conditional_probs = {}\n",
    "    \n",
    "    for (word1, word2), bigram_count in bigram_counter.items():\n",
    "        unigram_count = unigram_counter[(word1,)]\n",
    "        conditional_probs[(word1, word2)] = bigram_count / unigram_count\n",
    "    \n",
    "    return conditional_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization Functions\n",
    "Functions for creating visualizations of the n-gram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wordcloud creation function\n",
    "def create_wordcloud(ngram_counter, n, output_path):\n",
    "    \"\"\"Create and save wordcloud for n-grams\"\"\"\n",
    "    # Convert ngrams to strings for wordcloud\n",
    "    word_freq = {' '.join(ngram): count for ngram, count in ngram_counter.items()}\n",
    "    \n",
    "    # Find appropriate font for Amharic - try multiple options\n",
    "    font_options = [\n",
    "        '/System/Library/Fonts/Supplemental/Noto Sans Ethiopic.ttc',\n",
    "        '/System/Library/Fonts/Supplemental/Kefa.ttc',\n",
    "        '/Library/Fonts/NotoSansEthiopic-Regular.ttf',\n",
    "        '/usr/share/fonts/truetype/noto/NotoSansEthiopic-Regular.ttf',\n",
    "        '/usr/share/fonts/truetype/abyssinica/AbyssinicaSIL-R.ttf',\n",
    "        'C:\\\\Windows\\\\Fonts\\\\NotoSansEthiopic-Regular.ttf',\n",
    "        '/System/Library/Fonts/Supplemental/Arial Unicode.ttf',\n",
    "        None\n",
    "    ]\n",
    "    \n",
    "    # Try to find a working font\n",
    "    font_path = None\n",
    "    for font in font_options:\n",
    "        if font and os.path.exists(font):\n",
    "            font_path = font\n",
    "            print(f\"Using font: {font_path}\")\n",
    "            break\n",
    "    \n",
    "    # Create wordcloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400,\n",
    "        background_color='white',\n",
    "        font_path=font_path,\n",
    "        regexp=r'\\S+',\n",
    "        collocations=False,\n",
    "        prefer_horizontal=1.0,\n",
    "        min_font_size=12,\n",
    "        max_font_size=100\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # Plot and save\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{n}-gram Word Cloud\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved word cloud to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sentence Analysis Functions\n",
    "Functions for analyzing and generating sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentence probability function\n",
    "def calculate_sentence_probability(sentence, ngram_models, n, verbose=False):\n",
    "    \"\"\"Calculate probability of a sentence using n-gram model with detailed logging\"\"\"\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nCalculating probability for sentence: '{sentence}'\")\n",
    "        print(f\"Tokenized to: {tokens}\")\n",
    "    \n",
    "    if len(tokens) < n:\n",
    "        if verbose:\n",
    "            print(f\"Sentence too short for {n}-gram model (needs at least {n} tokens, has {len(tokens)})\")\n",
    "        return 0.0\n",
    "    \n",
    "    ngram_probs = ngram_models[n]['probs']\n",
    "    log_prob = 0.0\n",
    "    ngram_probs_list = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        \n",
    "        if ngram in ngram_probs:\n",
    "            prob = ngram_probs[ngram]\n",
    "            if verbose:\n",
    "                print(f\"  N-gram '{' '.join(ngram)}' found with prob={prob:.10f}\")\n",
    "        else:\n",
    "            prob = 1e-10\n",
    "            if verbose:\n",
    "                print(f\"  N-gram '{' '.join(ngram)}' not found, using smoothing value: {prob:.10f}\")\n",
    "                \n",
    "        log_prob += np.log(prob)\n",
    "        ngram_probs_list.append(prob)\n",
    "    \n",
    "    final_prob = np.exp(log_prob)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Log probability sum: {log_prob:.4f}\")\n",
    "        print(f\"Final probability: {final_prob:.10f}\")\n",
    "        print(f\"Calculation: exp({log_prob:.4f}) = {final_prob:.10f}\")\n",
    "        \n",
    "    return final_prob\n",
    "\n",
    "# %% Define sentence generation function\n",
    "def generate_random_sentence(ngram_models, n, max_length=20, verbose=False):\n",
    "    \"\"\"Generate a random sentence using n-gram model with detailed logging\"\"\"\n",
    "    ngram_counter = ngram_models[n]['counter']\n",
    "    starting_ngrams = [ngram for ngram in ngram_counter.keys() if ngram[0].istitle()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nGenerating random sentence using {n}-gram model\")\n",
    "        print(f\"Found {len(starting_ngrams)} capitalized starting n-grams\")\n",
    "    \n",
    "    if not starting_ngrams:\n",
    "        starting_ngrams = list(ngram_counter.keys())\n",
    "        if verbose:\n",
    "            print(\"No capitalized n-grams found, using random n-grams instead\")\n",
    "    \n",
    "    weights = [ngram_counter[ngram] for ngram in starting_ngrams]\n",
    "    starting_ngram = random.choices(starting_ngrams, weights=weights, k=1)[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Selected starting {n}-gram: '{' '.join(starting_ngram)}'\")\n",
    "    \n",
    "    generated_words = list(starting_ngram)\n",
    "    \n",
    "    for i in range(max_length - n):\n",
    "        context = tuple(generated_words[-(n-1):])\n",
    "        \n",
    "        if verbose and i < 5:\n",
    "            print(f\"\\nStep {i+1}: Current context: '{' '.join(context)}'\")\n",
    "        \n",
    "        possible_next = []\n",
    "        for ngram in ngram_counter:\n",
    "            if ngram[:-1] == context:\n",
    "                possible_next.append((ngram[-1], ngram_counter[ngram]))\n",
    "        \n",
    "        if not possible_next:\n",
    "            if verbose:\n",
    "                print(\"No continuation found for this context, ending sentence\")\n",
    "            break\n",
    "        \n",
    "        next_words, next_weights = zip(*possible_next)\n",
    "        next_word = random.choices(next_words, weights=next_weights, k=1)[0]\n",
    "        \n",
    "        if verbose and i < 5:\n",
    "            print(f\"Found {len(possible_next)} possible next words\")\n",
    "            top_3 = sorted(possible_next, key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(f\"Top 3 candidates: {[word for word, _ in top_3]}\")\n",
    "            print(f\"Selected next word: '{next_word}'\")\n",
    "        \n",
    "        generated_words.append(next_word)\n",
    "        \n",
    "        if next_word.endswith(('.', '?', '!')):\n",
    "            if verbose:\n",
    "                print(f\"Ending sentence due to punctuation: '{next_word}'\")\n",
    "            break\n",
    "    \n",
    "    sentence = ' '.join(generated_words)\n",
    "    if verbose:\n",
    "        print(f\"\\nGenerated sentence: '{sentence}'\")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation Functions\n",
    "Functions for evaluating model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation functions\n",
    "def intrinsic_evaluation(test_sentences, ngram_models):\n",
    "    \"\"\"Evaluate language models using perplexity\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in ngram_models:\n",
    "        if n == 1:  # Skip unigrams as they don't provide context\n",
    "            continue\n",
    "            \n",
    "        model = ngram_models[n]\n",
    "        ngram_probs = model['probs']\n",
    "        \n",
    "        total_log_prob = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            tokens = tokenize_sentence(sentence)\n",
    "            \n",
    "            if len(tokens) >= n:\n",
    "                for i in range(len(tokens) - n + 1):\n",
    "                    ngram = tuple(tokens[i:i+n])\n",
    "                    prob = ngram_probs.get(ngram, 1e-10)\n",
    "                    total_log_prob += np.log2(prob)\n",
    "                    total_tokens += 1\n",
    "        \n",
    "        perplexity = 2 ** (-total_log_prob / total_tokens) if total_tokens > 0 else float('inf')\n",
    "        results[n] = perplexity\n",
    "        \n",
    "    return results\n",
    "\n",
    "def extrinsic_evaluation(test_sentences, ngram_models):\n",
    "    \"\"\"Evaluate language models on next word prediction task\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for n in ngram_models:\n",
    "        if n == 1:\n",
    "            continue\n",
    "            \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            tokens = tokenize_sentence(sentence)\n",
    "            \n",
    "            if len(tokens) >= n:\n",
    "                for i in range(len(tokens) - n):\n",
    "                    context = tuple(tokens[i:i+n-1])\n",
    "                    actual_next_word = tokens[i+n-1]\n",
    "                    \n",
    "                    best_next_word = None\n",
    "                    best_prob = 0\n",
    "                    \n",
    "                    for ngram, prob in ngram_models[n]['probs'].items():\n",
    "                        if ngram[:-1] == context:\n",
    "                            if prob > best_prob:\n",
    "                                best_prob = prob\n",
    "                                best_next_word = ngram[-1]\n",
    "                    \n",
    "                    if best_next_word == actual_next_word:\n",
    "                        correct_predictions += 1\n",
    "                    total_predictions += 1\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        results[n] = accuracy\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Main Execution\n",
    "Main function to run the entire pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "    \n",
    "# Use full corpus\n",
    "data_path = FULL_DATA_PATH\n",
    "verbose = True\n",
    "\n",
    "# Read corpus and split data\n",
    "sentences = read_corpus(data_path)\n",
    "split_idx = int(len(sentences) * 0.9)\n",
    "train_sentences = sentences[:split_idx]\n",
    "test_sentences = sentences[split_idx:]\n",
    "\n",
    "print(f\"Training on {len(train_sentences)} sentences, testing on {len(test_sentences)} sentences.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stopwords(STOPWORDS_PATH)\n",
    "print(f\"Loaded {len(stopwords)} stopwords.\")\n",
    "if verbose and stopwords:\n",
    "    print(\"Sample stopwords:\")\n",
    "    print(', '.join(list(stopwords)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.1: Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.1: Creating n-grams ===\")\n",
    "ngram_models = {}\n",
    "\n",
    "for n in range(1, 5):\n",
    "    print(f\"\\nGenerating {n}-grams...\")\n",
    "    \n",
    "    # Without stopword removal\n",
    "    ngram_counter = compute_ngram_frequencies(train_sentences, n)\n",
    "    total_ngrams = sum(ngram_counter.values())\n",
    "    \n",
    "    # Log detailed examples to understand n-gram structure\n",
    "    if verbose:\n",
    "        log_ngram_examples(ngram_counter, n)\n",
    "    \n",
    "    ngram_probs = compute_ngram_probabilities(ngram_counter, total_ngrams)\n",
    "    \n",
    "    # Store models\n",
    "    ngram_models[n] = {\n",
    "        'counter': ngram_counter,\n",
    "        'probs': ngram_probs,\n",
    "        'total': total_ngrams\n",
    "    }\n",
    "    \n",
    "    print(f\"Generated {len(ngram_counter)} unique {n}-grams from {total_ngrams} total {n}-grams.\")\n",
    "    \n",
    "    # Print sample of n-grams\n",
    "    print(f\"Sample of most common {n}-grams:\")\n",
    "    for ngram, count in ngram_counter.most_common(5):\n",
    "        print(f\"  {' '.join(ngram)}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.2: Calculate probabilities and find top 10 n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.2: Top 10 most likely n-grams ===\")\n",
    "for n in range(1, 5):\n",
    "    print(f\"Top 10 most likely {n}-grams:\")\n",
    "    top_ngrams = sorted(ngram_models[n]['probs'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for ngram, prob in top_ngrams:\n",
    "        print(f\"  {' '.join(ngram)}: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.3: Calculate conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.3: Conditional probabilities using bigrams ===\")\n",
    "cond_probs = compute_conditional_probabilities(\n",
    "    ngram_models[2]['counter'],\n",
    "    ngram_models[1]['counter']\n",
    ")\n",
    "\n",
    "print(\"Sample of conditional probabilities P(word2|word1):\")\n",
    "for (word1, word2), prob in sorted(cond_probs.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  P({word2}|{word1}): {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.4: Remove stopwords and recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.4: N-grams after stopword removal ===\")\n",
    "ngram_models_no_stopwords = {}\n",
    "\n",
    "for n in range(1, 5):\n",
    "    print(f\"Generating {n}-grams without stopwords...\")\n",
    "    \n",
    "    # With stopword removal\n",
    "    ngram_counter = compute_ngram_frequencies(train_sentences, n, remove_stopwords=True, stopwords=stopwords)\n",
    "    total_ngrams = sum(ngram_counter.values())\n",
    "    ngram_probs = compute_ngram_probabilities(ngram_counter, total_ngrams)\n",
    "    \n",
    "    # Store models\n",
    "    ngram_models_no_stopwords[n] = {\n",
    "        'counter': ngram_counter,\n",
    "        'probs': ngram_probs,\n",
    "        'total': total_ngrams\n",
    "    }\n",
    "    \n",
    "    print(f\"Generated {len(ngram_counter)} unique {n}-grams from {total_ngrams} total {n}-grams.\")\n",
    "    \n",
    "    # Top 10 n-grams after stopword removal\n",
    "    print(f\"Top 10 most common {n}-grams after stopword removal:\")\n",
    "    for ngram, count in ngram_counter.most_common(10):\n",
    "        print(f\"  {' '.join(ngram)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.5: Create word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.5: Creating word clouds ===\")\n",
    "for n in range(1, 4):  # Unigrams, bigrams, trigrams\n",
    "    # Word cloud with stopwords\n",
    "    print(f\"Creating word cloud for {n}-grams with stopwords...\")\n",
    "    create_wordcloud(\n",
    "        ngram_models[n]['counter'],\n",
    "        n,\n",
    "        os.path.join(OUTPUT_DIR, f\"{n}gram_wordcloud.png\")\n",
    "    )\n",
    "    \n",
    "    # Word cloud without stopwords\n",
    "    print(f\"Creating word cloud for {n}-grams without stopwords...\")\n",
    "    create_wordcloud(\n",
    "        ngram_models_no_stopwords[n]['counter'],\n",
    "        n,\n",
    "        os.path.join(OUTPUT_DIR, f\"{n}gram_no_stopwords_wordcloud.png\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.6: Calculate sentence probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.6: Sentence probability ===\")\n",
    "sample_sentences = [\n",
    "    \"ኢትዮጵያ ታሪካዊ ሀገር ናት\",\n",
    "    \"አዲስ አበባ የኢትዮጵያ ዋና ከተማ ነው\",\n",
    "    \"የአማርኛ ቋንቋ ብዙ ተናጋሪዎች አሉት\"\n",
    "]\n",
    "\n",
    "for sentence in sample_sentences:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    for n in range(1, 5):\n",
    "        prob = calculate_sentence_probability(sentence, ngram_models, n, verbose=verbose)\n",
    "        print(f\"  Probability using {n}-gram model: {prob:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.7: Generate random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 1.7: Random sentence generation ===\")\n",
    "for n in range(2, 5):  # Bigrams, trigrams, 4-grams\n",
    "    print(f\"Generating sentences using {n}-gram model:\")\n",
    "    for i in range(3):\n",
    "        # Use verbose mode for the first sentence of each n\n",
    "        sentence = generate_random_sentence(ngram_models, n, verbose=(verbose and i==0))\n",
    "        print(f\"  {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Intrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 2: Intrinsic evaluation (perplexity) ===\")\n",
    "perplexities = intrinsic_evaluation(test_sentences, ngram_models)\n",
    "\n",
    "for n, perplexity in perplexities.items():\n",
    "    print(f\"  {n}-gram model perplexity: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Extrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Task 3: Extrinsic evaluation (next word prediction) ===\")\n",
    "accuracies = extrinsic_evaluation(test_sentences, ngram_models)\n",
    "\n",
    "for n, accuracy in accuracies.items():\n",
    "    print(f\"  {n}-gram model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
